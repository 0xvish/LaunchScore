# -*- coding: utf-8 -*-
"""Startup_Success_2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P6UrfDhJFrpdiJOagb1aplYMPZqcEZZx
"""

!pip install langchain kagglehub faiss-cpu sentence-transformers openai

!pip install langchain_community

import kagglehub
import pandas as pd

# Download dataset
path = kagglehub.dataset_download("omkargowda/indian-startups-funding-data")
print("Path to dataset files:", path)

# # Load CSV
df = pd.read_csv(f"{path}/startup_funding2021.csv")
df = df.dropna(subset=["What it does"])
df = df.reset_index(drop=True)

# # Show a preview
df[["Company/Brand", "What it does", "Amount($)", "Stage"]].head()

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Use MiniLM for embedding
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

texts = df["What it does"].tolist()
metadatas = df.to_dict("records")

vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)
vectorstore.save_local("startup_faiss_index")

retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Example label: 1 if Series A+ or amount > $1M, else 0
df['is_success'] = pd.to_numeric(df['Amount($)'].str.replace('[\$,]', '', regex=True), errors='coerce').fillna(0) > 1000000
df['is_success'] |= df['Stage'].fillna('').str.contains('Series [A-Z]', regex=True)
df['is_success'] = df['is_success'].astype(int)

from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

features = ['Sector', 'Stage', 'HeadQuarter']
X = df[features]
y = df['is_success']

preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), features)
])

model = Pipeline([
    ('preprocess', preprocessor),
    ('clf', LogisticRegression())
])

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)
model.fit(X_train, y_train)

import os
os.environ["GOOGLE_API_KEY"] = "AIzaSyCI6r8rXI0TvtQ5JsuiQAZ2LbXXskrqvcI"  # ‚Üê Replace with your key

!pip install langchain_experimental langchain-google-genai

from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="models/gemini-1.5-flash-latest",
    temperature=0.5
)

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a startup venture analyst with deep knowledge of market trends, startup funding, and investor behavior.

You are provided with descriptions and funding data of similar startups:

{context}

A new startup idea has been proposed:

"{question}"

Your tasks:
1. Analyze the idea and compare it with the retrieved startups.
2. Evaluate market demand, originality, competition, and funding trends.
3. Predict its likelihood of success on a scale of 0 to 10.
4. Justify the score with a brief analysis including potential risks and advantages.

Format your response like this:

Success Score: <score>/10
Reasoning:
- <Insight 1>
- <Insight 2>
- ...

Be clear, concise, and objective.
"""
)


qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt}
)

idea = "A blockchain-based platform to verify college degrees globally"
llm_response = qa_chain.run(idea)

# Extract score
import re
match = re.search(r'(\d+(?:\.\d+)?)\/10', llm_response)
llm_score = float(match.group(1)) if match else None

# Manually mapped for now
idea_features = pd.DataFrame([{
    'Sector': 'EdTech',
    'Stage': 'Seed',
    'HeadQuarter': 'Mumbai'
}])

ml_score = model.predict_proba(idea_features)[0][1] * 10  # Convert to 0‚Äì10 scale

final_score = 0.5 * llm_score + 0.5 * ml_score
print(f"LLM Score: {llm_score:.1f}, ML Score: {ml_score:.1f}, Final Score: {final_score:.1f}/10")

import matplotlib.pyplot as plt

plt.bar(['LLM', 'ML', 'Blended'], [llm_score, ml_score, final_score], color=['blue', 'green', 'purple'])
plt.title('Hybrid Startup Success Score')
plt.ylabel('Score (0‚Äì10)')
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

# Predict on test set
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("AUC-ROC Score:", roc_auc_score(y_test, y_proba))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)

plt.figure(figsize=(6, 6))
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.grid(False)
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, y_proba):.2f}', color='green')
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import precision_recall_curve

precision, recall, _ = precision_recall_curve(y_test, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.', color='darkorange')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True)
plt.show()

import joblib

# Save model to file
joblib.dump(model, 'ml_model.pkl')
print("ML model saved successfully.")

import joblib

# Save the complete pipeline
joblib.dump(model, 'ml_pipeline.pkl')
print("‚úÖ ML pipeline saved successfully.")

vectorstore.save_local("models/startup_faiss_index")
print("‚úÖ FAISS index saved at models/startup_faiss_index/")

import pandas as pd
import joblib
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

# === Load CSV ===
df = pd.read_csv("startup_funding2021.csv")

# === Basic Cleaning ===
# Drop rows missing descriptions or amount
df = df.dropna(subset=["What it does", "Amount($)", "Stage", "Sector", "HeadQuarter"])
df = df.reset_index(drop=True)

# Clean the "Amount($)" column: remove $ and commas
df["Amount($)"] = df["Amount($)"].astype(str)
df["Amount($)"] = df["Amount($)"].str.replace(r"[\$,]", "", regex=True)

# Convert to numeric and handle errors
df["Amount($)"] = pd.to_numeric(df["Amount($)"], errors='coerce')
df = df.dropna(subset=["Amount($)"])

# === Simulate Success Target ===
# Assume Success if funding is greater than $1M
df["Success"] = df["Amount($)"].apply(lambda x: 1 if x >= 1_000_000 else 0)

# === Features and Target ===
X = df[["Sector", "Stage", "HeadQuarter"]]
y = df["Success"]

# === Train-test split ===
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# === Preprocessing: One-hot encode categorical features ===
categorical_features = ["Sector", "Stage", "HeadQuarter"]
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)

# === ML Model ===
model = RandomForestClassifier(n_estimators=100, random_state=42)

# === Pipeline: Preprocessing + Model ===
pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", model)
])

# === Train ===
pipeline.fit(X_train, y_train)

# === Save the full pipeline ===
joblib.dump(pipeline, "ml_pipeline.pkl")

print("‚úÖ Training complete. Pipeline saved to models/ml_pipeline.pkl")

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import re
import joblib
import os

# ========== Load and Clean Data ==========
df = pd.read_csv("startup_funding2021.csv")

# Drop unusable columns
df = df.drop(columns=["Company/Brand", "What it does", "Founders", "Investor"])

# Clean amount column
def clean_amount(val):
    if pd.isna(val) or val == "$Undisclosed":
        return None
    val = val.replace("$", "").replace(",", "").strip()
    match = re.match(r"([\d\.]+)([a-zA-Z]*)", val)
    if match:
        number = float(match.group(1))
        unit = match.group(2).lower()
        multiplier = {"k": 1e3, "l": 1e5, "m": 1e6, "cr": 1e7, "b": 1e9}.get(unit, 1)
        return number * multiplier
    return None

df["Amount_clean"] = df["Amount($)"].apply(clean_amount)
df["Amount_clean"] = df["Amount_clean"].fillna(df["Amount_clean"].median())

# Handle missing values
df["Founded"] = df["Founded"].fillna(df["Founded"].median())
df["HeadQuarter"] = df["HeadQuarter"].fillna("Unknown")

# Label encoding for categories
sector_vocab = {sector: idx for idx, sector in enumerate(df["Sector"].dropna().unique())}
hq_vocab = {hq: idx for idx, hq in enumerate(df["HeadQuarter"].unique())}

df["sector_idx"] = df["Sector"].map(sector_vocab)
df["hq_idx"] = df["HeadQuarter"].map(hq_vocab)

# Save vocab for future inference
joblib.dump(sector_vocab, "models/sector_vocab.pkl")
joblib.dump(hq_vocab, "models/hq_vocab.pkl")

# Create a binary target: High funding = success
df["Success"] = (df["Amount_clean"] > 1e6).astype(int)

# Final features and labels
features = df[["sector_idx", "hq_idx", "Founded", "Amount_clean"]]
labels = df["Success"]

# Normalize numerical columns
scaler = MinMaxScaler()
features[["Founded", "Amount_clean"]] = scaler.fit_transform(features[["Founded", "Amount_clean"]])
joblib.dump(scaler, "models/nn_scaler.pkl")

# Split data
X_train, X_test, y_train, y_test = train_test_split(features.values, labels.values, test_size=0.2, random_state=42)

# ========== PyTorch Dataset and Model ==========
class StartupDataset(torch.utils.data.Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_ds = StartupDataset(X_train, y_train)
test_ds = StartupDataset(X_test, y_test)

train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32)

class StartupSuccessNN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

model = StartupSuccessNN(input_dim=4)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ========== Training Loop ==========
EPOCHS = 20
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        preds = model(batch_X)
        loss = criterion(preds, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}")

# ========== Evaluation ==========
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        preds = model(X_batch).round()
        correct += (preds == y_batch).sum().item()
        total += y_batch.size(0)
print(f"‚úÖ Accuracy on test set: {correct/total:.2%}")

# ========== Save Model ==========
os.makedirs("models", exist_ok=True)
torch.save(model.state_dict(), "models/startup_nn.pt")
print("‚úÖ Model saved to models/startup_nn.pt")

import torch
import torch.nn as nn
import joblib
import numpy as np

# ==== Load model + vocab + scaler ====
class StartupSuccessNN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

model = StartupSuccessNN(input_dim=4)
model.load_state_dict(torch.load("models/startup_nn.pt"))
model.eval()

sector_vocab = joblib.load("models/sector_vocab.pkl")
hq_vocab = joblib.load("models/hq_vocab.pkl")
scaler = joblib.load("models/nn_scaler.pkl")

# ==== Example Input ====
# Feel free to change this to test!
idea_input = {
    "sector": "IT",
    "headquarter": "Bangalore",
    "founded": 2020,
    "amount": 100000000  # $5M
}

# ==== Preprocess Input ====
sector_idx = sector_vocab.get(idea_input["sector"], 0)
hq_idx = hq_vocab.get(idea_input["headquarter"], 0)
founded = idea_input["founded"]
amount = idea_input["amount"]

X = np.array([[sector_idx, hq_idx, founded, amount]])
X[:, 2:] = scaler.transform(X[:, 2:])  # normalize founded + amount

X_tensor = torch.tensor(X, dtype=torch.float32)

# ==== Make Prediction ====
with torch.no_grad():
    prob = model(X_tensor).item()
    success_score = round(prob * 10, 2)

print(f"üîç Predicted Success Score: {success_score}/10")

